# Оценка качества

Правильная оценка - ключ к пониманию качества рекомендательной системы. В этой главе мы рассмотрим метрики, их интерпретацию и best practices.

## Класс Evaluator

Основной инструмент для оценки моделей:

```python
from recommender import Evaluator

evaluator = Evaluator(
    metrics=['precision', 'recall', 'ndcg', 'map', 'mrr', 'hit_rate'],
    k_values=[5, 10, 20, 50]
)

# Оценка модели
results = evaluator.evaluate(
    model=model,
    test_data=test,
    task='ranking',  # 'ranking' или 'rating_prediction'
    exclude_train=True,
    train_data=train
)

# Красивый вывод
evaluator.print_results(results)

# Доступ к метрикам
ndcg_10 = results['ndcg@10']
print(f"NDCG@10: {ndcg_10:.4f}")
```

## Ranking метрики

### Precision@K

Доля релевантных items в top-K рекомендациях:

\[
\text{Precision@K} = \frac{|\text{Recommended@K} \cap \text{Relevant}|}{K}
\]

```python
# Пример
# User 1:
#   Recommended: [10, 20, 30, 40, 50]  (K=5)
#   Relevant (test): [10, 30, 60, 70]
#   Precision@5 = 2/5 = 0.4
```

**Интерпретация:**
- 0.2 @ K=10: 2 релевантных item из 10 → OK
- 0.5 @ K=10: 5 релевантных items из 10 → Отлично
- Выше для малых K, ниже для больших K

### Recall@K

Доля найденных релевантных items из всех релевантных:

\[
\text{Recall@K} = \frac{|\text{Recommended@K} \cap \text{Relevant}|}{|\text{Relevant}|}
\]

```python
# Пример
# User 1:
#   Recommended: [10, 20, 30, 40, 50]  (K=5)
#   Relevant: [10, 30, 60, 70]  (4 items)
#   Recall@5 = 2/4 = 0.5
```

**Интерпретация:**
- 0.1 @ K=10: Нашли 10% релевантных → Плохо
- 0.3 @ K=10: Нашли 30% релевантных → OK
- 0.5+ @ K=10: Нашли 50%+ → Хорошо

### NDCG@K (Normalized Discounted Cumulative Gain)

Учитывает позицию item в списке рекомендаций:

\[
\text{NDCG@K} = \frac{DCG@K}{IDCG@K}
\]

где:
\[
DCG@K = \sum_{i=1}^{K} \frac{rel_i}{\log_2(i+1)}
\]

```python
# Пример
# Recommended: [10, 20, 30, 40, 50]
# Relevant: [10, 30, 60, 70]
# 
# DCG = 1/log2(2) + 0/log2(3) + 1/log2(4) + 0/log2(5) + 0/log2(6)
#     = 1.0 + 0 + 0.5 + 0 + 0 = 1.5
```

**Интерпретация:**
- **Лучшая метрика для ranking**
- 0.3-0.4: Хорошая модель
- 0.4-0.5: Очень хорошая
- 0.5+: Excellent

### MAP@K (Mean Average Precision)

Средняя точность по всем позициям:

\[
\text{MAP@K} = \frac{1}{|U|} \sum_{u=1}^{|U|} \frac{1}{\min(K, |Rel_u|)} \sum_{k=1}^{K} Precision@k \cdot rel(k)
\]

**Когда использовать:** Когда важен порядок всех релевантных items.

### MRR (Mean Reciprocal Rank)

Обратный ранг первого релевантного item:

\[
\text{MRR} = \frac{1}{|U|} \sum_{u=1}^{|U|} \frac{1}{rank_u}
\]

```python
# Примеры:
# User 1: Первый релевантный на позиции 1 → RR = 1.0
# User 2: Первый релевантный на позиции 3 → RR = 0.33
# User 3: Нет релевантных в top-K → RR = 0.0
# MRR = (1.0 + 0.33 + 0.0) / 3 = 0.44
```

**Когда использовать:** Когда важно найти хотя бы один релевантный item быстро.

### Hit Rate@K

Доля пользователей с хотя бы одним релевантным item в top-K:

\[
\text{Hit Rate@K} = \frac{|\{u : |Recommended_u@K \cap Relevant_u| > 0\}|}{|U|}
\]

**Интерпретация:**
- 0.5 @ K=10: 50% пользователей нашли хотя бы один релевантный
- 0.8 @ K=20: 80% пользователей → Хорошо

## Rating Prediction метрики

### RMSE (Root Mean Squared Error)

\[
\text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (r_i - \hat{r}_i)^2}
\]

```python
from recommender import SVDRecommender, Evaluator

model = SVDRecommender(n_components=50)
model.fit(train.data)

evaluator = Evaluator(metrics=['rmse', 'mae'])
results = evaluator.evaluate(model, test, task='rating_prediction')

print(f"RMSE: {results['rmse']:.4f}")
```

**Интерпретация (для рейтингов 1-5):**
- RMSE < 0.9: Отлично
- RMSE 0.9-1.0: Хорошо
- RMSE > 1.0: Требуется улучшение

### MAE (Mean Absolute Error)

\[
\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |r_i - \hat{r}_i|
\]

**Интерпретация:** Средняя ошибка в единицах рейтинга.

### MSE (Mean Squared Error)

\[
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (r_i - \hat{r}_i)^2
\]

### R² (Coefficient of Determination)

\[
R^2 = 1 - \frac{\sum (r_i - \hat{r}_i)^2}{\sum (r_i - \bar{r})^2}
\]

## Beyond-Accuracy метрики

### Coverage

Доля items, которые система может рекомендовать:

```python
def calculate_coverage(recommendations, n_items):
    """
    Args:
        recommendations: Dict[user_id, List[item_id]]
        n_items: Общее количество items
    """
    recommended_items = set()
    for user_recs in recommendations.values():
        recommended_items.update([item_id for item_id, _ in user_recs])
    
    coverage = len(recommended_items) / n_items
    return coverage

# Использование
recs = model.recommend(all_user_ids, k=10)
coverage = calculate_coverage(recs, dataset.n_items)
print(f"Coverage: {coverage:.2%}")
```

**Интерпретация:**
- < 10%: Популярные items доминируют
- 10-30%: OK для большинства систем
- > 50%: Хорошая диверсификация

### Diversity

Средняя непохожесть между рекомендованными items:

```python
from sklearn.metrics.pairwise import cosine_similarity

def calculate_diversity(recommendations, item_embeddings):
    """
    Args:
        recommendations: Dict[user_id, List[item_id]]
        item_embeddings: Embeddings items
    """
    diversities = []
    
    for user_id, items in recommendations.items():
        item_ids = [item_id for item_id, _ in items]
        if len(item_ids) < 2:
            continue
        
        # Embeddings рекомендованных items
        recs_embeddings = item_embeddings[item_ids]
        
        # Попарная similarity
        sim_matrix = cosine_similarity(recs_embeddings)
        
        # Diversity = 1 - average similarity
        avg_sim = (sim_matrix.sum() - len(item_ids)) / (len(item_ids) * (len(item_ids) - 1))
        diversity = 1 - avg_sim
        
        diversities.append(diversity)
    
    return np.mean(diversities)
```

### Novelty

Насколько неожиданные (не популярные) рекомендации:

```python
def calculate_novelty(recommendations, item_popularity):
    """
    Args:
        recommendations: Dict[user_id, List[item_id]]
        item_popularity: Dict[item_id, count]
    """
    novelties = []
    
    for user_id, items in recommendations.items():
        for item_id, _ in items:
            pop = item_popularity.get(item_id, 0)
            novelty = -np.log2(pop + 1)  # Логарифмическая шкала
            novelties.append(novelty)
    
    return np.mean(novelties)
```

## Cross-Validation

```python
from recommender import cross_validate

# K-fold CV
cv_results = cross_validate(
    model_class=EASERecommender,
    dataset=dataset,
    n_folds=5,
    metrics=['precision', 'recall', 'ndcg'],
    k_values=[10, 20],
    l2_reg=500.0  # Гиперпараметры модели
)

# Результаты для каждого fold
for fold_idx, results in enumerate(cv_results):
    print(f"Fold {fold_idx + 1}: NDCG@10 = {results['ndcg@10']:.4f}")

# Средние результаты
avg_ndcg = np.mean([r['ndcg@10'] for r in cv_results])
std_ndcg = np.std([r['ndcg@10'] for r in cv_results])
print(f"\nAverage NDCG@10: {avg_ndcg:.4f} ± {std_ndcg:.4f}")
```

## Интерпретация результатов

### Сравнение моделей

```python
models = {
    'EASE': EASERecommender(l2_reg=500.0),
    'ALS': ALSRecommender(n_factors=50, n_iterations=15),
    'NCF': NCFRecommender(embedding_dim=64, epochs=20)
}

evaluator = Evaluator(metrics=['precision', 'recall', 'ndcg'], k_values=[10])
results = {}

for name, model in models.items():
    model.fit(train.data)
    results[name] = evaluator.evaluate(model, test, task='ranking', train_data=train)

# Сравнение
print("\nСравнение моделей:")
print(f"{'Model':<10} {'Precision@10':<15} {'Recall@10':<15} {'NDCG@10':<15}")
print("-" * 55)
for name in models:
    p = results[name]['precision@10']
    r = results[name]['recall@10']
    n = results[name]['ndcg@10']
    print(f"{name:<10} {p:<15.4f} {r:<15.4f} {n:<15.4f}")
```

### Статистическая значимость

```python
from scipy import stats

def compare_models(results_a, results_b, metric='ndcg@10', n_folds=5):
    """
    T-test для сравнения моделей.
    
    Args:
        results_a, results_b: Результаты CV для двух моделей
        metric: Метрика для сравнения
    """
    scores_a = [r[metric] for r in results_a]
    scores_b = [r[metric] for r in results_b]
    
    t_stat, p_value = stats.ttest_rel(scores_a, scores_b)
    
    print(f"Model A: {np.mean(scores_a):.4f} ± {np.std(scores_a):.4f}")
    print(f"Model B: {np.mean(scores_b):.4f} ± {np.std(scores_b):.4f}")
    print(f"T-statistic: {t_stat:.4f}")
    print(f"P-value: {p_value:.4f}")
    
    if p_value < 0.05:
        if np.mean(scores_a) > np.mean(scores_b):
            print("✅ Model A significantly better (p < 0.05)")
        else:
            print("✅ Model B significantly better (p < 0.05)")
    else:
        print("⚠️  No significant difference (p >= 0.05)")
```

### Precision-Recall Curve

```python
import matplotlib.pyplot as plt

# Оценить на разных K
k_values = [5, 10, 20, 50, 100]
precisions = []
recalls = []

for k in k_values:
    evaluator = Evaluator(metrics=['precision', 'recall'], k_values=[k])
    results = evaluator.evaluate(model, test, task='ranking', train_data=train)
    precisions.append(results[f'precision@{k}'])
    recalls.append(results[f'recall@{k}'])

# График
plt.figure(figsize=(8, 6))
plt.plot(recalls, precisions, 'bo-')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.grid(True)

# Аннотации K
for i, k in enumerate(k_values):
    plt.annotate(f'K={k}', (recalls[i], precisions[i]))

plt.savefig('precision_recall_curve.png')
plt.close()
```

## Best Practices

### 1. Используйте правильную метрику для задачи

```python
# Ranking задача → NDCG, Recall
evaluator = Evaluator(metrics=['ndcg', 'recall'], k_values=[10, 20])

# Rating prediction → RMSE, MAE  
evaluator = Evaluator(metrics=['rmse', 'mae'])

# Diversity важна → Coverage, Diversity, Novelty
# Вычисляйте отдельно
```

### 2. Множественные K

```python
# Оценивайте на разных K
evaluator = Evaluator(metrics=['ndcg'], k_values=[5, 10, 20, 50])
```

### 3. Train/Val/Test split

```python
# Train: обучение
# Val: подбор гиперпараметров
# Test: финальная оценка (ОДИН раз!)

train, val, test = dataset.split(test_size=0.2, val_size=0.1)
```

### 4. Exclude train data

```python
# Всегда исключайте train items из рекомендаций
results = evaluator.evaluate(
    model, test,
    task='ranking',
    exclude_train=True,  # Важно!
    train_data=train
)
```

### 5. Мониторьте несколько метрик

```python
# Не только одна метрика
evaluator = Evaluator(
    metrics=['precision', 'recall', 'ndcg', 'hit_rate'],
    k_values=[10, 20]
)
```

## Что дальше?

- **[Продакшн решения](09_продакшн.md)** - FAISS, ensemble, REST API
- **[Примеры](10_примеры.md)** - End-to-end примеры использования

---

**Предыдущая глава**: [← Инференс](07_инференс.md)  
**Следующая глава**: [Продакшн решения →](09_продакшн.md)

