# Инференс

В этой главе мы рассмотрим генерацию рекомендаций в production: от базовых методов до оптимизаций для высоконагруженных систем.

## Генерация рекомендаций

### Метод recommend()

Основной метод для получения рекомендаций:

```python
from recommender import EASERecommender

model = EASERecommender(l2_reg=500.0)
model.fit(train.data)

# Рекомендации для одного пользователя
recommendations = model.recommend([1], k=10, exclude_seen=True)

# Формат возврата: Dict[user_id, List[Tuple[item_id, score]]]
for user_id, items in recommendations.items():
    print(f"User {user_id}:")
    for item_id, score in items[:5]:
        print(f"  Item {item_id}: score = {score:.3f}")
```

### Параметры recommend()

```python
recommendations = model.recommend(
    user_ids=[1, 2, 3],        # Список пользователей
    k=10,                       # Количество рекомендаций
    exclude_seen=True          # Исключить просмотренные items
)
```

#### `user_ids`
- **Тип**: List или numpy array
- **Описание**: IDs пользователей для рекомендаций

#### `k`
- **Тип**: int
- **Описание**: Количество рекомендаций на пользователя
- **Рекомендация**: 10-20 для UI, 100-1000 для re-ranking

#### `exclude_seen`
- **Тип**: bool
- **Описание**: Исключить items с которыми пользователь взаимодействовал
- **Рекомендация**: True для большинства случаев

### Метод predict()

Предсказание scores для конкретных пар user-item:

```python
# Предсказать scores
user_ids = [1, 1, 2, 2]
item_ids = [10, 20, 30, 40]

scores = model.predict(user_ids, item_ids)
print(scores)  # [0.85, 0.62, 0.91, 0.45]

# Для explicit ratings (SVD, SVD++)
predicted_ratings = svd_model.predict(user_ids, item_ids)
print(predicted_ratings)  # [4.2, 3.8, 4.5, 3.1]
```

**Когда использовать:**
- Re-ranking кандидатов
- A/B testing
- Предсказание explicit рейтинга

## Batch Inference

Для множества пользователей одновременно.

### BatchInference класс

```python
from recommender.utils import BatchInference

# Обернуть модель в BatchInference
batch_model = BatchInference(
    model=model,
    batch_size=128  # Обрабатывать по 128 пользователей
)

# Рекомендации для многих пользователей
user_ids = list(range(1, 1001))  # 1000 пользователей
recommendations = batch_model.recommend(user_ids, k=10)

print(f"Получено рекомендаций для {len(recommendations)} пользователей")
```

### Преимущества batching

```python
import time
import numpy as np

# Без batching
user_ids = list(range(1, 1001))
start = time.time()
for user_id in user_ids:
    recs = model.recommend([user_id], k=10)
no_batch_time = time.time() - start

# С batching
start = time.time()
batch_model = BatchInference(model, batch_size=128)
recs = batch_model.recommend(user_ids, k=10)
batch_time = time.time() - start

print(f"Без batching: {no_batch_time:.2f}s")
print(f"С batching: {batch_time:.2f}s")
print(f"Ускорение: {no_batch_time / batch_time:.1f}x")
```

### Параллельный batch inference

```python
from concurrent.futures import ThreadPoolExecutor

def get_recommendations_chunk(user_chunk):
    return model.recommend(user_chunk, k=10)

# Разбить пользователей на чанки
user_ids = list(range(1, 10001))
chunk_size = 1000
user_chunks = [user_ids[i:i+chunk_size] for i in range(0, len(user_ids), chunk_size)]

# Параллельная обработка
with ThreadPoolExecutor(max_workers=4) as executor:
    results = list(executor.map(get_recommendations_chunk, user_chunks))

# Объединить результаты
all_recommendations = {}
for chunk_result in results:
    all_recommendations.update(chunk_result)

print(f"Получено {len(all_recommendations)} рекомендаций")
```

## Оптимизация производительности

### 1. Кэширование (InferenceCache)

```python
from recommender.utils import InferenceCache

# Создать кэш
cache = InferenceCache(
    max_size=10000,  # Максимум элементов в кэше
    ttl=3600         # Time-to-live в секундах (1 час)
)

def get_recommendations_with_cache(user_id, k=10):
    cache_key = (user_id, k)
    
    # Проверить кэш
    cached = cache.get(cache_key)
    if cached is not None:
        print(f"✅ Cache hit for user {user_id}")
        return cached
    
    # Если нет в кэше - вычислить
    print(f"❌ Cache miss for user {user_id}")
    recommendations = model.recommend([user_id], k=k)
    
    # Сохранить в кэш
    cache.put(cache_key, recommendations)
    
    return recommendations

# Использование
recs1 = get_recommendations_with_cache(1, k=10)  # Cache miss
recs2 = get_recommendations_with_cache(1, k=10)  # Cache hit! (мгновенно)
```

### 2. Профилирование (profile_inference)

```python
from recommender.utils import profile_inference

@profile_inference
def get_batch_recommendations(user_ids, k=10):
    return model.recommend(user_ids, k=k)

# Вызовы автоматически профилируются
recs = get_batch_recommendations([1, 2, 3], k=10)
recs = get_batch_recommendations(list(range(1, 101)), k=10)

# Вывести статистику
get_batch_recommendations.print_stats()
# Outputs:
# get_batch_recommendations:
#   Calls: 2
#   Total time: 1.234s
#   Avg time: 0.617s
#   Min time: 0.005s
#   Max time: 1.229s
```

### 3. Предвычисление embeddings

Для deep learning моделей:

```python
# Для NCF, LightGCN - embeddings можно предвычислить
user_embeddings = model.get_user_embeddings()  # (n_users, embedding_dim)
item_embeddings = model.get_item_embeddings()  # (n_items, embedding_dim)

# Сохранить
import numpy as np
np.save('user_embeddings.npy', user_embeddings)
np.save('item_embeddings.npy', item_embeddings)

# Быстрый inference: dot product
def fast_recommend(user_id, k=10):
    user_emb = user_embeddings[user_id]
    scores = item_embeddings @ user_emb
    top_k_indices = np.argsort(scores)[::-1][:k]
    return [(idx, scores[idx]) for idx in top_k_indices]
```

## Фильтрация уже просмотренных items

### exclude_seen=True

Автоматическая фильтрация:

```python
# Автоматически исключает items из training data
recommendations = model.recommend(
    [1, 2, 3],
    k=10,
    exclude_seen=True  # По умолчанию
)
```

### Кастомная фильтрация

```python
def filter_recommendations(recommendations, user_history):
    """
    Дополнительная фильтрация.
    
    Args:
        recommendations: Dict[user_id, List[Tuple[item_id, score]]]
        user_history: Dict[user_id, Set[item_id]]
    """
    filtered = {}
    
    for user_id, items in recommendations.items():
        seen = user_history.get(user_id, set())
        # Фильтровать
        filtered_items = [(item_id, score) for item_id, score in items 
                         if item_id not in seen]
        filtered[user_id] = filtered_items
    
    return filtered

# Использование
user_history = {
    1: {10, 20, 30},  # User 1 видел items 10, 20, 30
    2: {15, 25},
    3: {5, 10, 15, 20}
}

recs = model.recommend([1, 2, 3], k=20, exclude_seen=True)
recs_filtered = filter_recommendations(recs, user_history)
```

### Фильтрация по правилам

```python
def filter_by_rules(recommendations, rules):
    """
    Фильтрация по бизнес-правилам.
    
    Правила могут быть:
    - Категории (не рекомендовать из определённых категорий)
    - Цена (не рекомендовать слишком дорогие)
    - Наличие (только в наличии)
    - Новизна (только новые товары)
    """
    # Пример: фильтр по категориям
    banned_categories = rules.get('banned_categories', set())
    
    filtered = {}
    for user_id, items in recommendations.items():
        valid_items = []
        for item_id, score in items:
            item_category = get_item_category(item_id)
            if item_category not in banned_categories:
                valid_items.append((item_id, score))
        filtered[user_id] = valid_items
    
    return filtered
```

## Cold Start проблема

### Для новых пользователей

Нет истории → нет персонализации.

**Решение 1: Popularity baseline**

```python
def get_popular_items(train_data, k=10):
    """Популярные items"""
    item_counts = train_data['item_id'].value_counts()
    top_items = item_counts.head(k).index.tolist()
    return top_items

# Для новых пользователей
def recommend_new_user(user_id, model, train_data, k=10):
    try:
        # Попробовать персонализированные рекомендации
        recs = model.recommend([user_id], k=k)
        return recs[user_id]
    except KeyError:
        # Новый пользователь → популярные items
        popular = get_popular_items(train_data, k=k)
        return [(item_id, 1.0) for item_id in popular]
```

**Решение 2: Feature-based модель**

```python
# Если есть features пользователя (возраст, пол, локация)
from sklearn.ensemble import RandomForestClassifier

# Обучить на известных пользователях
user_features = ...  # Features известных пользователей
user_preferences = ...  # Их предпочтения

clf = RandomForestClassifier()
clf.fit(user_features, user_preferences)

# Предсказать для нового пользователя
new_user_features = ...
predicted_preferences = clf.predict_proba(new_user_features)
```

**Решение 3: Гибридный подход**

```python
def hybrid_recommend(user_id, model, train_data, k=10, user_features=None):
    """
    Гибридные рекомендации:
    - Если есть история → персонализированные
    - Если мало истории → микс персонализированных + популярных
    - Если нет истории → популярные или feature-based
    """
    user_history_size = len(train_data[train_data['user_id'] == user_id])
    
    if user_history_size >= 10:
        # Достаточно истории → персонализированные
        return model.recommend([user_id], k=k)[user_id]
    
    elif user_history_size > 0:
        # Мало истории → микс
        personalized = model.recommend([user_id], k=k//2)[user_id]
        popular = get_popular_items(train_data, k=k//2)
        popular_with_scores = [(item_id, 0.5) for item_id in popular]
        return personalized + popular_with_scores
    
    else:
        # Нет истории → популярные
        popular = get_popular_items(train_data, k=k)
        return [(item_id, 1.0) for item_id in popular]
```

### Для новых items

Нет взаимодействий → не появляется в рекомендациях.

**Решение 1: Exploration**

```python
def recommend_with_exploration(user_id, model, all_items, k=10, exploration_rate=0.1):
    """
    Добавить новые items для исследования.
    """
    # Персонализированные рекомендации
    n_exploit = int(k * (1 - exploration_rate))
    recs = model.recommend([user_id], k=n_exploit)[user_id]
    
    # Новые items для исследования
    n_explore = k - n_exploit
    recommended_items = {item_id for item_id, _ in recs}
    new_items = [item for item in all_items if item not in recommended_items]
    explored = random.sample(new_items, min(n_explore, len(new_items)))
    explored_with_scores = [(item_id, 0.0) for item_id in explored]
    
    return recs + explored_with_scores
```

**Решение 2: Content-based для новых items**

```python
# Если есть features item'ов
from sklearn.metrics.pairwise import cosine_similarity

item_features = ...  # Features всех items (включая новые)

def recommend_similar_to_new_item(new_item_id, item_features, k=10):
    """Найти похожие items по features"""
    new_item_vec = item_features[new_item_id]
    similarities = cosine_similarity([new_item_vec], item_features)[0]
    top_k = np.argsort(similarities)[::-1][1:k+1]  # Исключить сам item
    return top_k.tolist()
```

## Мониторинг inference в production

### Логирование запросов

```python
import logging
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def recommend_with_logging(user_id, model, k=10):
    start_time = time.time()
    
    try:
        recommendations = model.recommend([user_id], k=k)
        latency = time.time() - start_time
        
        logger.info(f"SUCCESS: user={user_id}, k={k}, latency={latency:.3f}s, n_recs={len(recommendations[user_id])}")
        
        return recommendations[user_id]
    
    except Exception as e:
        latency = time.time() - start_time
        logger.error(f"ERROR: user={user_id}, k={k}, latency={latency:.3f}s, error={str(e)}")
        raise
```

### Метрики производительности

```python
from collections import defaultdict
import time

class InferenceMetrics:
    def __init__(self):
        self.latencies = []
        self.errors = 0
        self.total_requests = 0
    
    def record_request(self, latency, is_error=False):
        self.total_requests += 1
        self.latencies.append(latency)
        if is_error:
            self.errors += 1
    
    def get_stats(self):
        return {
            'total_requests': self.total_requests,
            'errors': self.errors,
            'error_rate': self.errors / self.total_requests if self.total_requests > 0 else 0,
            'avg_latency': np.mean(self.latencies) if self.latencies else 0,
            'p50_latency': np.percentile(self.latencies, 50) if self.latencies else 0,
            'p95_latency': np.percentile(self.latencies, 95) if self.latencies else 0,
            'p99_latency': np.percentile(self.latencies, 99) if self.latencies else 0,
        }

metrics = InferenceMetrics()

def recommend_with_metrics(user_id, model, k=10):
    start = time.time()
    try:
        recs = model.recommend([user_id], k=k)
        latency = time.time() - start
        metrics.record_request(latency, is_error=False)
        return recs[user_id]
    except Exception as e:
        latency = time.time() - start
        metrics.record_request(latency, is_error=True)
        raise

# Использование
for user_id in range(1, 1001):
    recs = recommend_with_metrics(user_id, model, k=10)

# Статистика
stats = metrics.get_stats()
print(f"Total requests: {stats['total_requests']}")
print(f"Error rate: {stats['error_rate']:.2%}")
print(f"Avg latency: {stats['avg_latency']:.3f}s")
print(f"P95 latency: {stats['p95_latency']:.3f}s")
```

## Best Practices

### 1. Всегда используйте batching

```python
# ❌ Плохо: по одному
for user_id in user_ids:
    recs = model.recommend([user_id], k=10)

# ✅ Хорошо: батчами
batch_size = 128
for i in range(0, len(user_ids), batch_size):
    batch = user_ids[i:i+batch_size]
    recs = model.recommend(batch, k=10)
```

### 2. Кэшируйте частые запросы

```python
# Для популярных пользователей
cache = InferenceCache(max_size=10000, ttl=3600)
```

### 3. Мониторьте производительность

```python
# Логируйте latency, error rate
# Алертинг если P95 > threshold
```

### 4. Graceful degradation

```python
def recommend_with_fallback(user_id, model, k=10, timeout=1.0):
    """Откат на популярные items при ошибках"""
    try:
        with timeout_context(timeout):
            return model.recommend([user_id], k=k)[user_id]
    except:
        # Fallback на популярные
        return get_popular_items(train_data, k=k)
```

### 5. A/B testing готовность

```python
def recommend_with_ab_test(user_id, model_a, model_b, k=10, ab_ratio=0.5):
    """A/B test между моделями"""
    import random
    
    if random.random() < ab_ratio:
        # Группа A
        recs = model_a.recommend([user_id], k=k)[user_id]
        variant = 'A'
    else:
        # Группа B
        recs = model_b.recommend([user_id], k=k)[user_id]
        variant = 'B'
    
    # Логировать вариант
    log_ab_test(user_id, variant, recs)
    
    return recs
```

## Что дальше?

- **[Оценка качества](08_оценка.md)** - метрики для оценки рекомендаций
- **[Продакшн решения](09_продакшн.md)** - FAISS, API, deployment

---

**Предыдущая глава**: [← Обучение](06_обучение.md)  
**Следующая глава**: [Оценка качества →](08_оценка.md)

