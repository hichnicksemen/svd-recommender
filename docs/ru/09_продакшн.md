# Продакшн Решения

В этой главе мы рассмотрим production-ready функции библиотеки: FAISS для быстрого поиска, ensemble моделей и REST API для deployment.

## FAISS Integration

FAISS (Facebook AI Similarity Search) - библиотека для эффективного поиска ближайших соседей.

### Зачем нужен FAISS?

**Проблема:** Для рекомендаций нужно найти top-K items с наибольшим score:

```python
# Наивный подход: O(n_items) для каждого пользователя
scores = user_embedding @ item_embeddings.T  # (n_items,)
top_k_indices = np.argsort(scores)[::-1][:k]
```

**Решение:** FAISS ускоряет поиск в 10-100 раз!

### Установка

```bash
# CPU версия
pip install faiss-cpu

# GPU версия (требуется CUDA)
pip install faiss-gpu
```

### Создание FAISS индекса

```python
from recommender.utils import FAISSIndex, create_faiss_index_from_model

# Предположим модель обучена (NCF, LightGCN, ALS)
model = NCFRecommender(...)
model.fit(train.data)

# Создать FAISS индекс из item embeddings
faiss_index = create_faiss_index_from_model(
    model,
    index_type='flat',  # 'flat', 'ivf', 'hnsw'
    use_gpu=False
)

print(f"✅ FAISS индекс создан: {faiss_index.n_items} items")
```

### Типы индексов

#### 1. Flat (Exact Search)

Точный поиск, медленнее но гарантированно правильно:

```python
faiss_index = create_faiss_index_from_model(
    model,
    index_type='flat',
    use_gpu=False
)

# O(n_items) но очень оптимизированно
```

#### 2. IVF (Inverted File Index)

Approximate поиск, быстрее:

```python
faiss_index = create_faiss_index_from_model(
    model,
    index_type='ivf',
    n_clusters=100,  # Количество кластеров
    n_probe=10,      # Сколько кластеров проверять
    use_gpu=False
)

# Быстрее в 10-50x, 95-99% точность
```

#### 3. HNSW (Hierarchical Navigable Small World)

Быстрый approximate поиск:

```python
faiss_index = create_faiss_index_from_model(
    model,
    index_type='hnsw',
    M=32,  # Количество связей
    use_gpu=False
)

# Очень быстро, хорошая точность
```

### Использование FAISS

```python
import numpy as np

# Получить user embedding
user_id = 1
user_idx = model.user_mapping[user_id]
user_embedding = model.user_factors[user_idx:user_idx+1]  # (1, embedding_dim)

# Поиск top-K items
k = 10
distances, item_indices = faiss_index.search(user_embedding, k=k)

# distances: (1, k) - расстояния
# item_indices: (1, k) - индексы items

# Конвертировать обратно в item IDs
top_items = []
for idx, dist in zip(item_indices[0], distances[0]):
    item_id = model.reverse_item_mapping[idx]
    score = 1 / (1 + dist)  # Конвертировать distance в score
    top_items.append((item_id, score))

print(f"Top-{k} items для User {user_id}:")
for item_id, score in top_items:
    print(f"  Item {item_id}: score = {score:.3f}")
```

### Сохранение и загрузка

```python
# Сохранить индекс
faiss_index.save('faiss_index.bin')

# Загрузить
from recommender.utils import FAISSIndex

loaded_index = FAISSIndex(embedding_dim=64)
loaded_index.load('faiss_index.bin')

# Использовать
distances, indices = loaded_index.search(user_embedding, k=10)
```

### GPU ускорение

```python
# Создать GPU индекс
faiss_index_gpu = create_faiss_index_from_model(
    model,
    index_type='flat',
    use_gpu=True  # Использовать GPU!
)

# Ускорение 10-100x по сравнению с CPU
```

### Benchmark

```python
import time

user_embeddings = model.user_factors[:1000]  # 1000 пользователей

# Без FAISS
start = time.time()
for user_emb in user_embeddings:
    scores = user_emb @ model.item_factors.T
    top_k = np.argsort(scores)[::-1][:10]
no_faiss_time = time.time() - start

# С FAISS
start = time.time()
distances, indices = faiss_index.search(user_embeddings, k=10)
faiss_time = time.time() - start

print(f"Без FAISS: {no_faiss_time:.2f}s")
print(f"С FAISS: {faiss_time:.2f}s")
print(f"Ускорение: {no_faiss_time / faiss_time:.1f}x")
```

## Model Ensemble

Комбинация нескольких моделей для лучшего качества.

### Создание Ensemble

```python
from recommender.utils import ModelEnsemble
from recommender import EASERecommender, ALSRecommender, NCFRecommender

# Обучить несколько моделей
ease = EASERecommender(l2_reg=500.0)
ease.fit(train.data)

als = ALSRecommender(n_factors=50, n_iterations=15)
als.fit(train.data)

ncf = NCFRecommender(embedding_dim=64, epochs=20)
ncf.fit(train.data)

# Создать ensemble
ensemble = ModelEnsemble(
    models=[ease, als, ncf],
    weights=[0.4, 0.3, 0.3],  # Веса моделей
    strategy='weighted_average'  # Стратегия комбинирования
)
```

### Стратегии комбинирования

#### 1. Weighted Average

```python
ensemble = ModelEnsemble(
    models=[ease, als],
    weights=[0.6, 0.4],
    strategy='weighted_average'
)

# Final score = 0.6 * ease_score + 0.4 * als_score
```

#### 2. Rank Fusion

```python
ensemble = ModelEnsemble(
    models=[ease, als],
    strategy='rank_fusion'
)

# Комбинирует по рангам, не scores
# Item на позиции 1 → rank score = 1.0
# Item на позиции 2 → rank score = 0.5
# и т.д.
```

#### 3. Stacking

```python
# Обучить meta-модель на выходах базовых моделей
# (требует дополнительной реализации)
```

### Использование Ensemble

```python
# Рекомендации от ensemble
recommendations = ensemble.recommend([1, 2, 3], k=10, exclude_seen=True)

for user_id, items in recommendations.items():
    print(f"\nUser {user_id}:")
    for item_id, score in items[:5]:
        print(f"  Item {item_id}: score = {score:.3f}")
```

### Подбор весов

```python
# Подобрать оптимальные веса на validation
from scipy.optimize import minimize

def objective(weights):
    ensemble = ModelEnsemble(
        models=[ease, als, ncf],
        weights=weights,
        strategy='weighted_average'
    )
    results = evaluator.evaluate(ensemble, val, task='ranking', train_data=train)
    return -results['ndcg@10']  # Минимизируем negative NDCG

# Оптимизация
initial_weights = [0.33, 0.33, 0.34]
constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}  # Сумма = 1
bounds = [(0, 1) for _ in range(3)]

result = minimize(objective, initial_weights, constraints=constraints, bounds=bounds)
best_weights = result.x

print(f"✅ Оптимальные веса: {best_weights}")
```

## REST API с FastAPI

Развёртывание модели как REST API сервис.

### Создание API

```python
from recommender.serving import create_service

# Сохранить модель
model.save('models/production_model.pkl')

# Создать API сервис
service = create_service(
    model_path='models/production_model.pkl',
    host='0.0.0.0',
    port=8000
)

# Запустить сервис
# service.run()
```

### Ручное создание API

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import uvicorn

app = FastAPI(title="Recommender API")

# Load model
from recommender import EASERecommender
model = EASERecommender()
model.load('models/production_model.pkl')

class RecommendRequest(BaseModel):
    user_ids: List[int]
    k: int = 10
    exclude_seen: bool = True

class RecommendResponse(BaseModel):
    recommendations: dict

@app.get("/")
def root():
    return {"message": "Recommender API", "version": "1.0"}

@app.get("/health")
def health():
    return {"status": "healthy"}

@app.post("/recommend", response_model=RecommendResponse)
def recommend(request: RecommendRequest):
    try:
        recommendations = model.recommend(
            request.user_ids,
            k=request.k,
            exclude_seen=request.exclude_seen
        )
        
        # Конвертировать в JSON-serializable format
        result = {}
        for user_id, items in recommendations.items():
            result[str(user_id)] = [
                {"item_id": int(item_id), "score": float(score)}
                for item_id, score in items
            ]
        
        return {"recommendations": result}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### Запуск API

```bash
# Способ 1: Python
python api.py

# Способ 2: Uvicorn
uvicorn api:app --host 0.0.0.0 --port 8000 --workers 4

# Способ 3: Docker (см. ниже)
```

### Использование API

```bash
# Health check
curl http://localhost:8000/health

# Рекомендации
curl -X POST "http://localhost:8000/recommend" \
  -H "Content-Type: application/json" \
  -d '{
    "user_ids": [1, 2, 3],
    "k": 10,
    "exclude_seen": true
  }'
```

Python клиент:

```python
import requests

# Получить рекомендации
response = requests.post(
    'http://localhost:8000/recommend',
    json={
        'user_ids': [1, 2, 3],
        'k': 10,
        'exclude_seen': True
    }
)

recommendations = response.json()['recommendations']

for user_id, items in recommendations.items():
    print(f"User {user_id}:")
    for item in items[:5]:
        print(f"  Item {item['item_id']}: {item['score']:.3f}")
```

## Deployment

### Docker

**Dockerfile:**

```dockerfile
FROM python:3.8-slim

WORKDIR /app

# Установка зависимостей
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Копирование кода и модели
COPY . .
COPY models/production_model.pkl models/

# Запуск API
EXPOSE 8000
CMD ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Сборка и запуск:**

```bash
# Сборка image
docker build -t recommender-api:v1 .

# Запуск контейнера
docker run -d -p 8000:8000 --name recommender recommender-api:v1

# Проверка
curl http://localhost:8000/health
```

### Docker Compose

**docker-compose.yml:**

```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - MODEL_PATH=/models/production_model.pkl
      - LOG_LEVEL=info
    volumes:
      - ./models:/models
    restart: always
    
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
```

```bash
# Запуск
docker-compose up -d

# Остановка
docker-compose down
```

### Kubernetes

**deployment.yaml:**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: recommender-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: recommender
  template:
    metadata:
      labels:
        app: recommender
    spec:
      containers:
      - name: api
        image: recommender-api:v1
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
---
apiVersion: v1
kind: Service
metadata:
  name: recommender-service
spec:
  selector:
    app: recommender
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

### Мониторинг

```python
from prometheus_client import Counter, Histogram, generate_latest
import time

# Метрики
request_count = Counter('recommender_requests_total', 'Total requests')
request_latency = Histogram('recommender_request_latency_seconds', 'Request latency')
errors = Counter('recommender_errors_total', 'Total errors')

@app.post("/recommend")
def recommend_with_metrics(request: RecommendRequest):
    request_count.inc()
    
    start = time.time()
    try:
        recommendations = model.recommend(request.user_ids, k=request.k)
        request_latency.observe(time.time() - start)
        return {"recommendations": recommendations}
    except Exception as e:
        errors.inc()
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/metrics")
def metrics():
    return generate_latest()
```

## Best Practices

### 1. Model Versioning

```python
# Сохранять модели с версией
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
model.save(f'models/ease_v1_{timestamp}.pkl')

# Метаданные
metadata = {
    'version': 'v1',
    'model': 'EASE',
    'timestamp': timestamp,
    'metrics': {'ndcg@10': 0.385}
}
```

### 2. A/B Testing

```python
@app.post("/recommend")
def recommend_ab(request: RecommendRequest, user_id: int):
    # Разделить пользователей на группы
    if user_id % 2 == 0:
        # Группа A: текущая модель
        recs = model_a.recommend([user_id], k=request.k)
        log_variant(user_id, 'A')
    else:
        # Группа B: новая модель
        recs = model_b.recommend([user_id], k=request.k)
        log_variant(user_id, 'B')
    
    return {"recommendations": recs}
```

### 3. Graceful Degradation

```python
@app.post("/recommend")
def recommend_with_fallback(request: RecommendRequest):
    try:
        # Попробовать персонализированные
        return model.recommend(request.user_ids, k=request.k)
    except Exception as e:
        logger.error(f"Model error: {e}")
        # Fallback на популярные items
        return get_popular_recommendations(request.k)
```

### 4. Caching

```python
from functools import lru_cache
import redis

# Redis cache
redis_client = redis.Redis(host='localhost', port=6379, db=0)

@app.post("/recommend")
def recommend_cached(request: RecommendRequest):
    cache_key = f"recs:{request.user_ids}:{request.k}"
    
    # Проверить cache
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Вычислить
    recs = model.recommend(request.user_ids, k=request.k)
    
    # Сохранить в cache (TTL 1 час)
    redis_client.setex(cache_key, 3600, json.dumps(recs))
    
    return recs
```

### 5. Rate Limiting

```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter

@app.post("/recommend")
@limiter.limit("10/minute")  # Макс 10 запросов в минуту
def recommend(request: RecommendRequest):
    return model.recommend(request.user_ids, k=request.k)
```

## Что дальше?

- **[Примеры](10_примеры.md)** - End-to-end примеры использования
- **[FAQ](11_faq.md)** - Часто задаваемые вопросы и troubleshooting

---

**Предыдущая глава**: [← Оценка качества](08_оценка.md)  
**Следующая глава**: [Примеры использования →](10_примеры.md)

